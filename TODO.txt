TODO

* right now we allow data blocks to have identical names. This is not a good idea. Fix.

* think about maintaining a list that is cutoff long, of the distances between
  closest subset pairs. 
  The list would need to be updated whenever we make new subsets. It might be
  a bit tough to do it, but it would address the argsort() function limitation, which is what currently slows us down the most. 

* implement 'search = auto'. This will save people from choosing a search 
  method. The idea is just to go with the defaults below, but it's a neat feature for PF2. We can time it against 1.1.1 as well, and show the improvements. It might also be possible to estimate the weights automatically and iteratively using the new arrays, but running a linear model that updates at each step to calculate weights. 

* Change the default settings to:
    --rcluster-percent 100
    --rcluster-max 1000

    This is a better setting for almost all cases, because it ensures that we 
    look hard for new merges when towards the end of the algorithm.
    And it means that we don't spend too long on any one step.
    A few preliminary tests show that it does way better than e.g. rcluster-percent 1.0 on large datasets, because that one tends not to look hard enough.

    As a bonus, this avoids having giant arrays, and so speeds up the algo.
    in general as well. 

* change user output so that % done is per step.

* More efficiency: create d and c matrices at the start of a run. 
  Make them (N + N-1) along each of 2 dimensions. Also make a list of subsets in the start 
  scheme (i.e. N subsets long). Populate those matrices as the algorithm progresses. And when 
  we're done with a subset, label the d and c columns and rows for that subset with np.inf.
  This is the most memory and speed efficient way to proceed, because numpy arrays populate
  contiguous memory blocks, and creating new arrays means copying ot a totally new piece of
  memory.  

* odd bug - we seem to have lost the ability to tell whether an analysis was run in phyml or raxml
  so we run the risk of taking results from shitty subsets. Need to add this to the old config
  comparison method. 

* Add in sensible dependency checks at the start, and print out useful error messages.
    e.g. "You need to have SciKit-Learn installed to run this analysis, to do that, 
    please install the anaconda python distribution from here..."

* Bug fix (kind of): if a user quits during the estimation of the starting tree, funny things
    happen. We need to put in some kind of check for this. Basically just make sure that
    if we're thinking of using old results, then we check that the starting tree file has
    something in it. If it doesn't, quit with an error message and ask the user to 
    use force-restart

* Usability: change the 'we aborted' error message to suggest trying force-restart. A lot of 
    user errors are fixed this way, there have been a few queries on email and on the user 
    group.

* merge phyml_models.py and raxml_models.py into one, have a 'model' class, make models
   know to which lists they belong, and let them return their own command lines when asked



Brett test enhancing:
---------------------

TODO 
* change name back to full_name in subset reporting
* Only makes sense to report ONE IC in the greedy algo output...
* make progress part of the reporter too
* change name back to full_name in subset reporting
* think about using __slots__ in subset and scheme?
* Consider dropping part_subsets in schemes (only need it for checking...)
* think about ditching schemes as we go, and subset info?


DONE
* Only makes sense to report ONE IC in the greedy algo output...
* make progress part of the reporter too
* Make the output match (why aren't schemes numbered the same...)
* move path creation into config
* make results member of scheme? or keep a paired list
* put all of the summarising of results info (finding best) into the AnalysisResults
* move subset writing out into reporter
* finish making reports fleshed out
* Final wiritng takes Analysis results
* think about reporter, how should it look? with config?
* USE Analysis results to produce a test_object_compartor.
* The write methods for comparining that into AnalysisResults (with epsilon differences)
* Do a folder diff on example folder, making sure that these changes
*


Done
____

* fix the reporting

* Check this line
        number_of_seq = len(self.alignment.species)
  in analysis.py. It looks wrong to me - I think it's for the AICc calculation, and it 
  suggests that we're calculating it with the number of species, not the number of sites in the 
  alignment. That's a bit worrying. And should be checked and probably fixed
* While I'm on the AIC/AICc/BIC, we should have a single function for these, called from 
    different places. Right now there are functions in subset and scheme. No need for that.
* fix file cleanup with raxml analyses. Right now there are a lot of files
  left over in the phylofiles folder. Not good.
* add 'clustering' search option to the manual
* add --additional_cmd_line option so that we can pass e.g. precision controllers to PhyML and RAxML
  and we could also then use the RAxML Pthreads version, and tell it how many processors to use.
  helpful for massive datasets on desktops I suspect.
* when we fail to load output from RAxML, we need to go and delete ALL files that contain the 
  run_id in question. Right now we're not doing this properly so running stuff again is failing...
* if we're not saving phylofiles, we need to do a better job deleting RAxML files more generally, right now 
  stuff is stacking up in the phyml folder. Best way to do that is as above, and maybe we should just add a function
  into phyml.py and raxml.py, something like delete_analysis_files(run_ID). That way we can keep things in check.
- add --raxml to manual, include all the model definitions
- the numbers of columns are wrong when we warn that there are missing columns. Need to add 1 to the final number.
- we don't save phyml output by default now, but we have the option to
- we have universal line break support now
- flag a warning if ANYTHING in the .cfg file has changed. Do this by putting a copy of the .cfg file in the 'analysis' folder, loading it and comparing it to the current one
- fixed all scheme and subset number calculators
- mrbayes and raxml now options for model spec in the config file
- Get better .cfg parsing, so that we get useful error messages
- semicolons now needed, fixes some bugs in .cfg input
- add a more efficient Bell-number counter when counting schemes
- Use all CPU's default (rather than 1)
- Only use the user schemes if 'search' == 'user'
- check that Brett's implementation of source alignment and filtered alignment works.
- build initial tree and branchlengths NOT from the source alignment, but from the un-partitioned alignment. These can be different e.g. if I exclude some sites from all of my partitions. This is really important.
- Ordered scheme output in partitions in codon order (in scheme.py.write_summary)
- implemented Greedy search algorithm. Right now it works on whatever metric you specify in model_selection
- tidied up output of all_schemes.txt, best_schemes.txt, and the subset output files too
- Added model_selection option, AIC, AICc, or BIC now implemented
- Added branch_lengths option 'linked' or 'unlinked'
- Fixed calling of PhyML so that it actually constrains brlens
- PhyML constrained_lens option fixed by Stephane
- Added an error message if you specify a partition with sites that aren't in the alignment
- Fixed partition definition to include single sites
- optional charset at beginning of partition def 
